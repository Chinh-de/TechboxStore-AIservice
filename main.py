# import json
# import lancedb
# import numpy as np
# import io
# import os
# from fastapi import FastAPI, File, UploadFile, HTTPException
# from pydantic import BaseModel
# from typing import List, Optional
# from PIL import Image

# # --- IMPORT MODELS ---
# from sentence_transformers import SentenceTransformer
# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
# from tensorflow.keras.preprocessing import image
# import google.generativeai as genai

# from dotenv import load_dotenv
# load_dotenv()


# app = FastAPI(title="AI Super Service: Search - Recs - Chatbot")

# # --- C·∫§U H√åNH CHUNG ---
# DB_PATH = "./lancedb_data"
# # GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") # ƒê·∫£m b·∫£o ƒë√£ set bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c thay tr·ª±c ti·∫øp
# GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# # config weight
# W_BRAND = 2.0  # ∆Øu ti√™n H√£ng
# W_CATE  = 2.0  # ∆Øu ti√™n Lo·∫°i
# W_SPECS = 1.5  # ∆Øu ti√™n C·∫•u h√¨nh
# W_DESC = 1.0

# # T√™n c√°c b·∫£ng trong LanceDB
# TBL_PRODUCTS = "products"        # Ch·ª©a vector Search & Recs
# TBL_IMAGES = "product_images"    # Ch·ª©a vector ·∫¢nh
# TBL_KNOWLEDGE = "knowledge_base"       # Ch·ª©a t√†i li·ªáu RAG (L∆∞u √Ω: T√™n b·∫£ng ph·∫£i kh·ªõp v·ªõi file seed_rag.py)

# # Bi·∫øn to√†n c·ª•c (Singleton)
# models = {}
# db_tables = {}

# # --- H√ÄM TI·ªÜN √çCH ---
# # def normalize(v):
# #     norm = np.linalg.norm(v)
# #     if norm == 0: return v
# #     return v / norm

# def process_image_to_vector(img_bytes):
#     img = Image.open(io.BytesIO(img_bytes))
#     if img.mode != 'RGB': img = img.convert('RGB')
#     img = img.resize((224, 224))
#     img_array  = image.img_to_array(img)
#     expanded_img_array = np.expand_dims(img_array, axis=0)
#     preprocessed_img = preprocess_input(expanded_img_array)
#     features  = models['image'].predict(preprocessed_img, verbose=0).flatten()
#     return features

# # --- KH·ªûI T·∫†O SERVER (CH·∫†Y 1 L·∫¶N) ---
# @app.on_event("startup")
# async def startup_event():
#     print(" ƒêang kh·ªüi ƒë·ªông AI Super Service...")
    
#     # 1. Load Text Model (D√πng chung cho c·∫£ Search v√† Chatbot)
#     print("   - Loading Text Model (dangvantuan)...")
#     models['text'] = SentenceTransformer('dangvantuan/vietnamese-document-embedding', trust_remote_code=True)
#     models['text'].max_seq_length = 4096
    
#     # 2. Load Image Model (ResNet50)
#     print("   - Loading Image Model (ResNet50)...")
#     models['image'] = ResNet50(weights='imagenet', include_top=False, pooling='avg')
    
#     # 3. Setup Gemini
#     print("   - Configuring Gemini...")
#     if GOOGLE_API_KEY:
#         genai.configure(api_key=GOOGLE_API_KEY)
#         models['gemini'] = genai.GenerativeModel('gemini-2.0-flash-lite')
#     else:
#         print(" Ch∆∞a c√≥ GOOGLE_API_KEY. Chatbot s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")

#     # 4. K·∫øt n·ªëi LanceDB
#     print("   - Connecting Database...")
#     db = lancedb.connect(DB_PATH)
#     existing_tables = db.table_names()
    
#     # M·ªü b·∫£ng Products
#     if TBL_PRODUCTS in existing_tables:
#         db_tables['products'] = db.open_table(TBL_PRODUCTS)
#     else:
#         print(f" Thi·∫øu b·∫£ng '{TBL_PRODUCTS}'. API Search/Recs s·∫Ω l·ªói.")

#     # M·ªü b·∫£ng Images
#     if TBL_IMAGES in existing_tables:
#         db_tables['images'] = db.open_table(TBL_IMAGES)
#     else:
#         print(f" Thi·∫øu b·∫£ng '{TBL_IMAGES}'. API Search Image s·∫Ω l·ªói.")

#     # M·ªü b·∫£ng Knowledge (Cho Chatbot)
#     if TBL_KNOWLEDGE in existing_tables:
#         db_tables['knowledge'] = db.open_table(TBL_KNOWLEDGE)
#     else:
#         if "knowledge_base" in existing_tables:
#              db_tables['knowledge'] = db.open_table("knowledge_base")
#         else:
#              print(f" Thi·∫øu b·∫£ng '{TBL_KNOWLEDGE}'. Chatbot s·∫Ω l·ªói.")

#     print(" H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")

# # --- DTOs ---
# class ChatMessage(BaseModel):
#     role: str
#     content: str

# class ChatRequest(BaseModel):
#     question: str
#     history: List[ChatMessage] = []

# class SearchRequest(BaseModel):
#     query: str
#     top_k: int = 10

# class RecRequest(BaseModel):
#     spus: List[str]
#     top_k: int = 10
# def route_question(user_query, history_text):
#     """
#     D√πng Gemini ƒë·ªÉ ph√¢n lo·∫°i c√¢u h·ªèi v√† t·ªëi ∆∞u h√≥a query.
#     Tr·∫£ v·ªÅ JSON: { "intent": "PRODUCT"|"POLICY"|"CHITCHAT", "optimized_query": "..." }
#     """
#     prompt = f"""
#     B·∫°n l√† b·ªô n√£o ph√¢n lo·∫°i tin nh·∫Øn cho TechStore.
    
#     L·ªäCH S·ª¨ CHAT:
#     {history_text}
    
#     C√ÇU KH√ÅCH H·ªéI: "{user_query}"
    
#     NHI·ªÜM V·ª§:
#     1. Ph√¢n lo·∫°i √Ω ƒë·ªãnh (intent) v√†o 1 trong 3 nh√≥m:
#        - POLICY: H·ªèi v·ªÅ c√°c ch·ªß ƒë·ªÅ: Kh√°ch H√†ng Doanh Nghi·ªáp & D·ª± √Ån, Ch√≠nh S√°ch Kh√°ch H√†ng, Ch√≠nh S√°ch Thanh To√°n & V·∫≠n Chuy·ªÉn, Thu C≈© ƒê·ªïi M·ªõi, B·∫£o H√†nh & ƒê·ªïi Tr·∫£, B·∫£o M·∫≠t Th√¥ng Tin, V·ªá Sinh & N√¢ng C·∫•p Thi·∫øt B·ªã, ƒêi·ªÅu Kho·∫£n D·ªãch V·ª•, H∆∞·ªõng D·∫´n Kh·∫Øc Ph·ª•c S·ª± C·ªë C∆° B·∫£n, Gi·ªõi Thi·ªáu V·ªÅ Techbox Store.
#        - PRODUCT: H·ªèi mua, t∆∞ v·∫•n, so s√°nh, m√¥ t·∫£ nhu c·∫ßu, t√¨m s·∫£n ph·∫©m (laptop, chu·ªôt, ph√≠m...).
#        - CHITCHAT: Ch√†o h·ªèi, c·∫£m ∆°n, tr√™u ƒë√πa, ho·∫∑c kh√¥ng li√™n quan mua b√°n.
       
#     2. Vi·∫øt l·∫°i c√¢u h·ªèi (optimized_query) ƒë·ªÉ t√¨m ki·∫øm t·ªët h∆°n:
#        - N·∫øu l√† PRODUCT: T√≥m t·∫Øt nhu c·∫ßu th√†nh keywords (VD: "M√°y r·∫ª" -> "Laptop gi√° r·∫ª d∆∞·ªõi 10 tri·ªáu").
#        - N·∫øu l√† POLICY: Vi·∫øt r√µ r√†ng (VD: "B·∫£o h√†nh ko?" -> "Ch√≠nh s√°ch b·∫£o h√†nh").
#        - N·∫øu l√† CHITCHAT: Gi·ªØ nguy√™n.
       
#     OUTPUT JSON FORMAT:
#     {{
#         "intent": "PRODUCT", 
#         "optimized_query": "Laptop gaming Dell d∆∞·ªõi 20 tri·ªáu"
#     }}
#     Ch·ªâ tr·∫£ v·ªÅ JSON thu·∫ßn, kh√¥ng markdown.
#     """
    
#     try:
#         response = models['gemini'].generate_content(prompt)
#         text = response.text.strip().replace('```json', '').replace('```', '')
#         return json.loads(text)
#     except:
#         # Fallback n·∫øu l·ªói JSON
#         return {"intent": "CHITCHAT", "optimized_query": user_query}




# def search_products(query: str, k: int =10, return_full_text: bool = True):
#     """
#     T√¨m ki·∫øm s·∫£n ph·∫©m v√† tr·∫£ v·ªÅ danh s√°ch chi ti·∫øt.
#     Output format: List[{'spu': str, 'full_text': str, 'score': float}]
#     """
#     if 'products' not in db_tables: 
#         return []
    
#     # Encode & Normalize
#     query_vec = models['text'].encode([query])[0]
#     # query_vec = normalize(query_vec)
    
#     # Search LanceDB
#     results = db_tables['products'].search(query_vec, vector_column_name="vector_search") \
#         .metric("cosine").limit(k).to_list()
        
#     structured_results = []
#     text_content = ""
#     for r in results:
#         # L·∫•y th√¥ng tin text ƒë·ªÉ Gemini ƒë·ªçc
#         if return_full_text:
#             text_content = r.get('full_text', '')
        
#         structured_results.append({
#             "spu": r['spu'],          # SPU ƒë·ªÉ tr·∫£ v·ªÅ Frontend
#             "full_text": text_content, # Text ƒë·ªÉ ƒë∆∞a v√†o Prompt
#             "score": 1 - r['_distance'] # ƒê·ªô gi·ªëng
#         })
    
#     return structured_results

# # ==========================================
# # API 1: T√åM KI·∫æM TEXT (Semantic Search)
# # ==========================================

# @app.post("/search/text")
# async def search_text(req: SearchRequest):
#     try:
#         results = search_products(
#             query=req.query, 
#             k=req.top_k, 
#             return_full_text=False 
#         )
        
#         return {"status": "success", "data": results}
        
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))

# # ==========================================
# # API 2: G·ª¢I √ù (Personalized Recommendation)
# # ==========================================
# @app.post("/recommend")
# async def recommend(req: RecRequest):
#     try:
#         if not req.spus: return {"data": []}

#         # 1. L·∫•y d·ªØ li·ªáu t·ª´ DB
#         # fixbug: LanceDB tr·∫£ v·ªÅ th·ª© t·ª± ng·∫´u nhi√™n
#         spus_str = ", ".join([f"'{s}'" for s in req.spus])
#         items = db_tables['products'].search().where(f"spu IN ({spus_str})").limit(len(req.spus)).to_list()
        
#         if not items: return {"data": []}

#         # 2. Map spu -> Vector ƒë·ªÉ ƒë·ªìng b·ªô th·ª© t·ª±
#         # Gi·∫£ s·ª≠ req.spus g·ª≠i l√™n theo th·ª© t·ª±: [C≈© nh·∫•t, ..., M·ªõi nh·∫•t]
#         spu_to_vec = {item['spu']: np.array(item['vector_recs']) for item in items}
        
#         ordered_vectors = []
#         found_spus = [] 
        
#         for spu in req.spus:
#             if spu in spu_to_vec:
#                 ordered_vectors.append(spu_to_vec[spu])
#                 found_spus.append(spu)
        
#         if not ordered_vectors: return {"data": []}

#         # 3. T√≠nh Vector Trung B√¨nh v·ªõi Tr·ªçng s·ªë Th·ªùi gian (Time Decay)
#         DECAY_FACTOR = 0.9
#         n = len(ordered_vectors)
        
#         # T·∫°o m·∫£ng tr·ªçng s·ªë: 
#         # i ch·∫°y t·ª´ 0 -> n-1. 
#         # i=0 (M·ªõi nh·∫•t) -> 0.9^0 = 1.0
#         # i=1 (C≈© h∆°n)   -> 0.9^1 = 0.9
#         weights = [DECAY_FACTOR ** i for i in range(n)]
        
#         # T√≠nh trung b√¨nh c√≥ tr·ªçng s·ªë
#         user_vec = np.average(ordered_vectors, axis=0, weights=weights)
        
#         # 4. Chu·∫©n h√≥a vector User
#         # user_vec = normalize(user_vec)
        
#         # 5. T√¨m ki·∫øm
#         # L·∫•y d∆∞ ra (top_k + s·ªë l∆∞·ª£ng l·ªãch s·ª≠) ƒë·ªÉ tr·ª´ hao
#         results = db_tables['products'].search(user_vec, vector_column_name="vector_recs") \
#             .metric("cosine") \
#             .limit(req.top_k + len(found_spus)) \
#             .to_list()
            
#         # 6. L·ªçc b·ªè s·∫£n ph·∫©m ƒë√£ c√≥
#         final = []
#         seen = set(found_spus)
        
#         for r in results:
#             if r['spu'] not in seen:
#                 final.append({
#                     "spu": r['spu'], 
#                     "score": 1 - r['_distance']
#                 })
#                 if len(final) >= req.top_k: break
                
#         return {"data": final}
        
#     except Exception as e:
#         print(f"Error Recommend: {e}")
#         raise HTTPException(500, str(e))

# # ==========================================
# # API 3: T√åM KI·∫æM ·∫¢NH (Image Search)
# # ==========================================
# @app.post("/search/image")
# async def search_image(file: UploadFile = File(...), top_k: int = 10):
#     try:
#         content = await file.read()
#         vec = process_image_to_vector(content)
        
#         results = db_tables['images'].search(vec).metric("cosine").limit(top_k*3).to_list()
        
#         final = []
#         seen = set()
#         for r in results:
#             if r['spu'] not in seen and r['spu'] != "UNKNOWN":
#                 final.append({"spu": r['spu'], "score": 1 - r['_distance']})
#                 seen.add(r['spu'])
#             if len(final) >= top_k: break
#         return {"data": final}
#     except Exception as e:
#         raise HTTPException(500, str(e))

# # ==========================================
# # API 4: CHATBOT RAG (Gemini + LanceDB)
# # ==========================================
# @app.post("/chat")
# async def chat_bot(req: ChatRequest):
#     if 'gemini' not in models: raise HTTPException(503, "Gemini not configured")
    
#     try:
#         # 1. Chu·∫©n b·ªã l·ªãch s·ª≠
#         hist_str = "\n".join([f"{m.role}: {m.content}" for m in req.history[-6:]])
        
#         # 2. G·ªåI ROUTER (Ph√¢n lo·∫°i √Ω ƒë·ªãnh)
#         router_res = route_question(req.question, hist_str)
#         intent = router_res.get("intent", "CHITCHAT")
#         search_query = router_res.get("optimized_query", req.question)
        
#         print(f"üîç Intent: {intent} | Query: {search_query}")
        
#         context_str = ""
#         system_instruction = ""
        
#         suggested_spus = []
#         src = ""
        
#         # 3. X·ª¨ L√ù THEO NH√ÅNH
#         if intent == "PRODUCT":
#             products_found = search_products(search_query, k=5)
            
#             if products_found:
#                 # A. T·∫°o Context cho Gemini (L·∫•y full_text)
#                 # Th√™m s·ªë th·ª© t·ª± ƒë·ªÉ Gemini d·ªÖ tr√≠ch d·∫´n
#                 prod_texts = [f"{i+1}. {p['full_text']}" for i, p in enumerate(products_found)]
#                 context_str = f"[DANH S√ÅCH S·∫¢N PH·∫®M PH√ô H·ª¢P]:\n" + "\n".join(prod_texts)
                
#                 # B. L·∫•y spu ƒë·ªÉ tr·∫£ v·ªÅ Frontend (L·∫•y spu)
#                 suggested_spus = [p['spu'] for p in products_found]
                
#                 system_instruction = "B·∫°n l√† nh√¢n vi√™n Sales. D·ª±a v√†o danh s√°ch s·∫£n ph·∫©m tr√™n ƒë·ªÉ t∆∞ v·∫•n, so s√°nh v√† m·ªùi kh√°ch mua c√°c s·∫£n ph·∫©m ƒëi k√®m."
#             else:
#                 context_str = "Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o kh·ªõp v·ªõi y√™u c·∫ßu."
#                 system_instruction = "Xin l·ªói kh√°ch v√† h·ªèi th√™m nhu c·∫ßu chi ti·∫øt h∆°n."
            
#         elif intent == "POLICY":
#             if 'knowledge' in db_tables:
#                 # q_vec = normalize(models['text'].encode([search_query])[0])
#                 q_vec = models['text'].encode([search_query])[0]
#                 res = db_tables['knowledge'].search(q_vec).metric("cosine").limit(5).to_list()

#                 src = "\n".join([f"- {r['source']}" for r in res])

#                 policy_text = "\n".join([f"- {r['text']}" for r in res])
#                 context_str = f"[TH√îNG TIN CH√çNH S√ÅCH]:\n{policy_text}"
#             system_instruction = "B·∫°n l√† nh√¢n vi√™n CSKH. Tr·∫£ l·ªùi th·∫Øc m·∫Øc d·ª±a tr√™n ch√≠nh s√°ch. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y b·∫£o kh√°ch g·ªçi hotline 1900 1234."
            
#         else: # CHITCHAT
#             system_instruction = "B·∫°n l√† tr·ª£ l√Ω ·∫£o TechStore th√¢n thi·ªán. H√£y tr√≤ chuy·ªán vui v·∫ª nh∆∞ng kh√©o l√©o l√°i v·ªÅ ch·ªß ƒë·ªÅ c√¥ng ngh·ªá."

        
#         # 4. T·∫†O PROMPT
#         final_prompt = f"""
#         {system_instruction}

#         ƒê·ªãnh d·∫°ng tr·∫£ v·ªÅ: Ch·ªâ tr·∫£ l·ªùi vƒÉn b·∫£n thu·∫ßn, kh√¥ng markdown.
        
#         D·ªÆ LI·ªÜU THAM KH·∫¢O:
#         {context_str}
        
#         L·ªäCH S·ª¨ CHAT:
#         {hist_str}
        
#         KH√ÅCH H·ªéI: "{req.question}"
#         TR·∫¢ L·ªúI:
#         """
        
#         # 5. GENERATE
#         response = models['gemini'].generate_content(final_prompt)
        
#         # 6. TR·∫¢ V·ªÄ K·∫æT QU·∫¢ (K√àM spu)
#         return {
#             "answer": response.text.strip(),
#             "intent": intent,
#             "related_products": suggested_spus,
#             "src" : src,
#             "debug_query": search_query
#         }

#     except Exception as e:
#         print(f"Error: {e}")
#         return {"answer": "Xin l·ªói, h·ªá th·ªëng ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau.", "intent": "ERROR", "related_products": []}

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)